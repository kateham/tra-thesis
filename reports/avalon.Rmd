---
title: "Corporate Landlords - Avalon"
author: "Kate Ham"
date: "7/17/2020"
output: 
  github_document:
    toc: true
---

## Preface

Idea for this report came from Tenants Together's manual guide for corporate landlord ownership lookup [pdf](https://www.tenantstogether.org/sites/tenantstogether.org/files/Wall_St_Landlord_Participatory_Action_Research_Guide.pdf). In short, the guide says to find a landlord's website and get all the listing addresses. Then, go to a proptech data source like Property Radar to check ownership information.

### Packages

This EDA tries to speed up this process by scraping all the listing addresses from a landlord site with [rvest](https://github.com/tidyverse/rvest), then using dyplr with pre-downloaded CoreLogic data to quickly get owner information.

While using rvest, use the [SelectorGadget](https://selectorgadget.com/) Chrome extension to get the CSS selector for the addresses. Save the result as `css_selector`.

We will also use the [postmastr](https://github.com/slu-openGIS/postmastr) package to parse the addresses. This package is still in development so needs to be downloaded remotely.

*Limitations: The CoreLogic data downloaded is from 2019 and is only for the 9-county Bay Area. In addition, this analysis will exclusively look at [Avalon's properties in Northern California](https://www.avaloncommunities.com/northern-california).*

*Warning: The `corelogic` file, which contains the CoreLogic data for the 9 counties in the Bay Area, is 3.6 GB.*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE}
## Libraries
library(tidyverse)
library(rvest)
library(lubridate)
library(knitr)
# install.packages("remotes")
#remotes::install_github("slu-openGIS/postmastr")
library(postmastr)

```

```{r parameters1}
## Parameters

### Webscraping
url_data <- "https://www.avaloncommunities.com/northern-california" # Accessed 7-17-2020
css_selector <- ".address"
```

```{r parameters2}
### CoreLogic
corelogic <- read_rds(here::here("data-raw/all9counties.rds"))
best_vars <- read_rds(here::here("data/best_vars.rds"))
file_data_dic <- here::here("data-raw/Bulk_Tax_Current_Layout w parcel level lat long_03102016.xlsx")
  # Property Indicator filter
RESIDENTIAL <- c(0:199)
  # recode function
recode_county <- function(county_fips) {
  county_fips %>% 
    recode(
      '06001' = "alameda",
      '06013' = "contra_costa", 
      '06041' = "marin",
      '06055' = "napa", 
      '06075' = "san_francisco",
      '06081' = "san_mateo",
      '06085' = "santa_clara",
      '06095' = "solano",
      '06097' = "sonoma" 
    )
}
```

## Avalon

### Webscraping with Rvest

First, scrape the data from the Avalon website using rvest and the proper CSS selector. Then, save the read in as HTML text.

Note: If the html elements are in lists, check out this [thread](https://stackoverflow.com/questions/52650604/scrapping-li-elements-with-rvest).

```{r read}
address_data <- 
  url_data %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
```

Next, save the address data in a tibble to make it easier to manage.

```{r addresses}
avalon <- 
  tibble(
    owner = "avalon",
    address = str_to_upper(address_data),
    access_date = Sys.Date()
  )
avalon %>% head() %>% kable()
```

From this webscrape, there are `r nrow(avalon)` listings on the Avalon site.

### Parsing Addresses with Postmastr

According to the postmastr [guide](https://slu-opengis.github.io/postmastr/articles/postmastr.html), We first need to create the appropriate dictionaries for elements of the address.

```{r dicts}
dict_ca <- 
  pm_dictionary(
    type = "state",
    filter = "CA",
    case = "upper",
    locale = "us"
  )

dict_cities <- 
  pm_dictionary(
    type = "city",
    filter = "CA",
    case = "upper",
    locale = "us"
  )

dict_dir <- 
  pm_dictionary(
    type = "directional",
    case = "upper"
  )

# I can't figure out how to tidy this...
dict_mode <- 
  pm_dictionary(
    type = "suffix",
    case = "upper"
  )
# get abbreviated street modes
dict_mode <- 
  dict_mode %>% 
  bind_rows(
    map(
      1, 
      ~ dict_mode %>% 
        mutate(suf.input = str_c(.$suf.input, "."))
    )
  )
```

We then parse out each element of the address and clean up the table a bit so it's easier to join with the CoreLogic data. Be sure to modify the function as necessary depending on the elements that are missing from the scraped addresses. There's definitely already some parsing errors that might be able to be fixed upon further examination.

```{r parse}
avalon <- 
  avalon %>% 
  pm_identify(var = "address") %>% 
  pm_parse(
    input = "full",
    address = "address",
    output = "full",
    keep_parsed = "yes",
    side = "right",
    keep_ids = TRUE,
    dir_dict = dict_dir,
    suffix_dict = dict_mode,
    city_dict = dict_cities,
    state_dict = dict_ca,
    locale = "us"
  ) %>%
  transmute(
    owner,
    address,
    access_date,
    house_num = pm.house,
    dir = str_to_upper(pm.preDir),
    street = str_to_upper(pm.street),
    mode = str_to_upper(pm.streetSuf),
    city = pm.city,
    zipcode = pm.zip
  )
avalon %>% head() %>% kable()
```

## CoreLogic

### Subsetting

The following code chunks clean and create a file of all the residential properties in the 9-county Bay Area with select variables (less than 25% missing values).

NOTE: Parsing failures are for properties without sales transaction recording dates.

```{r subset}
cl_select <-
  corelogic %>%
  select(all_of(best_vars)) %>% 
  filter(LAND_USE %in% RESIDENTIAL) %>% 
  mutate(RECORDING_DATE = ymd(RECORDING_DATE)) %>% 
  mutate(county = recode_county(FIPS_CODE))
```

Feel free to check the `file_data_dic` for more information. There are `r length(names(cl_select))` different variables, though these variables will be of most interest in joining with CL data:

```{r select names}
cl_select %>% 
  select(contains("OWNER"), starts_with("SITUS"), starts_with("PARCEL_LEVEL")) %>% 
  head() %>% kable()
```

### Finding Owners

Join the Avalon properties data `avalon` with the CoreLogic data `cl_select` to get more information about the properties. 

```{r join}
avalon_cl <- 
  avalon %>% 
  inner_join(
    cl_select, 
    by = 
      c(
        "house_num" = "SITUS_HOUSE_NUMBER", 
        "street" = "SITUS_STREET_NAME",
        "city" = "SITUS_CITY"
      )
  )
avalon_cl %>% head() %>% kable()
```

There are `r length(avalon_cl)` Avalon properties in the CoreLogic dataset.

Many properties have seemingly duplicate data. This is because the Avalon data is by property building and the CL data is by individual unit. For the purposes of this analysis, we assume that all of a building's units have the same owner. However, I left them all in the tibble to maximize the property information available.

Only Avalon properties that matched with the CL data were kept. Ones that did not match were likely due to parsing errors with postmastr. We will ignore the unmatched properties, but you can see how many there are from the chunk below.

```{r unjoined}
avalon %>% 
  anti_join(
    cl_select, 
    by = 
      c(
        "house_num" = "SITUS_HOUSE_NUMBER", 
        "street" = "SITUS_STREET_NAME",
        "city" = "SITUS_CITY"
      )
  ) %>% 
  nrow()
```

The following is a list of all the owner names that Avalon goes by. 

```{r owners}
owners <- 
  avalon_cl %>% 
  distinct(OWNER1_LAST_NAME) %>% 
  pull()
owners %>% kable()
```

There are `r length(owners)` different names.

The following are units owned by Avalon that were not listed on their website.

```{r unlisted owners}
avalon_unlisted <- 
  cl_select %>% 
  filter(OWNER1_LAST_NAME %in% owners) %>% 
  anti_join(avalon_cl, by = "FORMATTED_APN") %>% 
  select(contains("SITUS"))
avalon_unlisted %>% head() %>% kable()
```

There are `r nrow(avalon_unlisted)` units. If we remove units with the same property address, we find that there are `r avalon_unlisted %>% distinct(SITUS_CITY, SITUS_STREET_NAME, SITUS_HOUSE_NUMBER) %>% nrow()` different properties.

## Conclusion

This basic process of scraping and joining with CoreLogic data is a method of speeding up the manual process as detailed in the original Tenants Together guide. Certainly the methods would need to be tweaked for individual websites. In further EDAs, I will need to explore using the `RSelenium` package, as many residential property company websites use cookies.
